{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14305c3f",
   "metadata": {},
   "source": [
    "\n",
    "#### The focus on this notebook is the two types of unsupervised data augmentation techqiues for training data that is applied to Logistic Regression. \n",
    "1) GMM and Random Forest Semi Supervised Data Labeling for Train leads to a 0.667 accuracy on Kaggle with Logistic Regression\n",
    "2) KM and Random Forest Semi Supervised data Labeling for Train leads to a 0.766 accuracy on Kaggle with Logistic Regression\n",
    "3) Logistic Regression fully supervised leads to 0.922 accuracy on Kaggle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f1efab",
   "metadata": {},
   "source": [
    "### GMM and Random Forest, Semi Supervised Data Labeling \n",
    "- **Data Preprocessing:** Load and clean text data, removing stop words, lemmatizing, and removing URLs and mentions.\n",
    "- **Vectorization and Encoding:** Convert text to TF-IDF vectors with 500 features; encode sentiment labels in the labeled data.\n",
    "- **Separate Labeled and Unlabeled Data:** Isolate labeled and unlabeled portions of the training dataset.\n",
    "- **Augment the training data set** with labels using GMM that have a confidence threshold of 0.99.\n",
    "    - Train GMM with 5 clusters on labeled data and apply it to predict clusters for unlabeled data.\n",
    "    - Retain high-confidence pseudo-labels with a probability threshold of 0.99.\n",
    "    - Augment the training data set with labels using Random Forest that have a confidence threshold of 0.95.\n",
    "- **Train Random Forest** on the labeled data and predict labels for remaining unlabeled data.\n",
    "    - Retain high-confidence pseudo-labels with a probability threshold of 0.95.\n",
    "- **Combine and Save Dataset**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1980b2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marianellasalinas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marianellasalinas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marianellasalinas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marianellasalinas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final combined dataset shape: (59678, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "def pre_process(data):\n",
    "    preproc_data = data.copy()\n",
    "    nltk.download('stopwords')\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    preproc_data = preproc_data.apply(lambda text: ' '.join([word for word in str(text).split() if word.lower() not in stop_words]))\n",
    "    nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    preproc_data = preproc_data.apply(lambda text: ' '.join([lemmatizer.lemmatize(word) for word in text.split()]))\n",
    "    preproc_data = preproc_data.apply(lambda text: re.sub(r'@\\w+', '', re.sub(r'http\\S+|www\\S+', '', text)))\n",
    "    return preproc_data\n",
    "\n",
    "\n",
    "# Call preprocess func\n",
    "val_df['Phrase'] = pre_process(val_df['Phrase'])\n",
    "train_df['Phrase'] = pre_process(train_df['Phrase'])\n",
    "\n",
    "# Encode the Sentiment labels and make vectorizer \n",
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Split the labeled and unlabeled data\n",
    "labeled_train_df = train_df[train_df['Sentiment'] != -100].copy()\n",
    "unlabeled_train_df = train_df[train_df['Sentiment'] == -100].copy() \n",
    "\n",
    "# Prepare labeled data and unlabeled data\n",
    "X_labeled = vectorizer.fit_transform(labeled_train_df['Phrase'])\n",
    "y_labeled = label_encoder.fit_transform(labeled_train_df['Sentiment'])\n",
    "X_unlabeled = vectorizer.transform(unlabeled_train_df['Phrase'])\n",
    "\n",
    "# Do Unsupervised Labeling with GMM\n",
    "n_clusters = len(np.unique(y_labeled))\n",
    "gmm = GaussianMixture(n_components=n_clusters, random_state=0)\n",
    "gmm.fit(X_labeled.toarray())\n",
    "gmm_labels = gmm.predict(X_unlabeled.toarray())\n",
    "gmm_probs = gmm.predict_proba(X_unlabeled.toarray())\n",
    "\n",
    "# Do a high confidence threshold for GMM predictions\n",
    "gmm_confidence_threshold = 0.99\n",
    "high_confidence_mask_gmm = gmm_probs.max(axis=1) > gmm_confidence_threshold\n",
    "high_confidence_gmm_df = unlabeled_train_df[high_confidence_mask_gmm].copy()\n",
    "high_confidence_gmm_df['Sentiment'] = label_encoder.inverse_transform(gmm_labels[high_confidence_mask_gmm])\n",
    "\n",
    "# Exclude GMM-labeled data from the Random Forest labeling \n",
    "unlabeled_train_df_remaining = unlabeled_train_df[~high_confidence_mask_gmm]\n",
    "\n",
    "# Supervised Labeling with Random Forest\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_labeled, y_labeled)\n",
    "\n",
    "# Predict labels and probabilities on the remaining unlabeled data\n",
    "X_unlabeled_remaining = vectorizer.transform(unlabeled_train_df_remaining['Phrase'])\n",
    "pseudo_labels_rf = rf.predict(X_unlabeled_remaining)\n",
    "pseudo_probs_rf = rf.predict_proba(X_unlabeled_remaining).max(axis=1)\n",
    "\n",
    "# Apply a confidence threshold for Random Forest\n",
    "rf_confidence_threshold = 0.95\n",
    "high_confidence_mask_rf = pseudo_probs_rf > rf_confidence_threshold\n",
    "high_confidence_rf_df = unlabeled_train_df_remaining[high_confidence_mask_rf].copy()\n",
    "high_confidence_rf_df['Sentiment'] = label_encoder.inverse_transform(pseudo_labels_rf[high_confidence_mask_rf])\n",
    "\n",
    "# Combine the original labeled data, high-confidence GMM-labeled data, and high-confidence RF-labeled data\n",
    "final_combined_df = pd.concat(\n",
    "    [labeled_train_df, high_confidence_gmm_df[['Phrase', 'Sentiment']], high_confidence_rf_df[['Phrase', 'Sentiment']]],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Save the final combined dataset\n",
    "final_combined_df.to_csv('gmm_rf_combined_submission.csv', index=False)\n",
    "print(\"Final combined dataset shape:\", final_combined_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc51e88",
   "metadata": {},
   "source": [
    "### KMeans Unsupervised Data Labeling \n",
    "- **Data Preprocessing**: Load and clean text data, removing unnecessary elements, lemmatizing, and applying the preprocessesing func.\n",
    "- **Encode the Sentiment Labels and Vectorize**: Convert text to TF-IDF vectors with 500 features and encode sentiment labels in labeled data.\n",
    "- **Split Labeled and Unlabeled Data**: Separate the labeled data from the unlabeled data in the training dataset.\n",
    "- **Augment the training data set** with labels using KMeans with a confidence threshold of 0.80\n",
    "  - Train KMeans with 5 clusters on labeled data and predict clusters for unlabeled data.\n",
    "  - Calculate distances from each unlabeled point to its closest cluster center and set a confidence threshold based on the 80th percentile of distance.\n",
    "  - Get high-confidence labels that meet this threshold.\n",
    "- **Map Clusters to Sentiments**: Assign each cluster to the most frequent sentiment in the labeled data, creating a mapping for labels.\n",
    "- **Combine and Save Dataset**: Merge the original labeled data with high-confidence KMeans labled data and save.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "825bbb26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marianellasalinas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marianellasalinas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/marianellasalinas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marianellasalinas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High-confidence predictions shape: (29705, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances_argmin_min\n",
    "\n",
    "# Load data\n",
    "val_df = pd.read_csv(\"val.csv\")\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "\n",
    "# Call preprocess func\n",
    "val_df['Phrase'] = pre_process(val_df['Phrase'])\n",
    "train_df['Phrase'] = pre_process(train_df['Phrase'])\n",
    "\n",
    "# Split labeled and unlabeled data\n",
    "labeled_train_df = train_df[train_df['Sentiment'] != -100].copy()\n",
    "unlabeled_train_df = train_df[train_df['Sentiment'] == -100].copy() \n",
    "combined_labeled_df = pd.concat([labeled_train_df], ignore_index=True)\n",
    "\n",
    "# Encode the Sentiment labels and Vectorize \n",
    "label_encoder = LabelEncoder()\n",
    "combined_labeled_df['Sentiment_encoded'] = label_encoder.fit_transform(combined_labeled_df['Sentiment'])\n",
    "vectorizer = TfidfVectorizer(max_features=500) \n",
    "X_combined_labeled = vectorizer.fit_transform(combined_labeled_df['Phrase'])\n",
    "X_unlabeled = vectorizer.transform(unlabeled_train_df['Phrase'])\n",
    "\n",
    "# Fit K-Means on labeled data and predict for unlabeled data\n",
    "n_clusters = len(combined_labeled_df['Sentiment_encoded'].unique())\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "kmeans.fit(X_combined_labeled)\n",
    "\n",
    "# Predict clusters and calculate distances for unlabeled data\n",
    "unlabeled_train_df.loc[:, 'Predicted_Cluster'] = kmeans.predict(X_unlabeled)  \n",
    "closest, distances = pairwise_distances_argmin_min(X_unlabeled, kmeans.cluster_centers_)\n",
    "\n",
    "# Get a threshold for high confidence\n",
    "confidence_threshold = np.percentile(distances, 85)\n",
    "unlabeled_train_df.loc[:, 'High_Confidence'] = distances < confidence_threshold  \n",
    "\n",
    "# Map clusters to sentiments\n",
    "cluster_sentiment_map = {}\n",
    "for i in range(n_clusters):\n",
    "    cluster_labels = combined_labeled_df['Sentiment_encoded'][kmeans.labels_ == i]\n",
    "    if len(cluster_labels) > 0:\n",
    "        cluster_sentiment_map[i] = mode(cluster_labels, keepdims=True).mode[0]\n",
    "    else:\n",
    "        cluster_sentiment_map[i] = -1\n",
    "\n",
    "unlabeled_train_df.loc[:, 'Predicted_Sentiment'] = label_encoder.inverse_transform(\n",
    "    unlabeled_train_df['Predicted_Cluster'].map(cluster_sentiment_map)\n",
    ")  \n",
    "\n",
    "# Only filter out high-confidence predictions\n",
    "high_confidence_df = unlabeled_train_df[unlabeled_train_df['High_Confidence']].copy()\n",
    "\n",
    "final_train_df = pd.concat([\n",
    "    labeled_train_df,\n",
    "    high_confidence_df[['Phrase', 'Predicted_Sentiment']].rename(columns={'Predicted_Sentiment': 'Sentiment'})\n",
    "], ignore_index=True)\n",
    "\n",
    "# Save the high-confidence labeled dataset\n",
    "final_train_df.to_csv('km_combined_submission.csv', index=False)\n",
    "print(\"High-confidence predictions shape:\", high_confidence_df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d09fc5",
   "metadata": {},
   "source": [
    "### Use the augmented training data to our unsupervised learning methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b57398df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaN values in X_train_clean_prep['Phrase']: 0\n",
      "Number of NaN values in X_val_prep['Phrase']: 0\n",
      "Number of NaN values in X_test_prep['Phrase']: 0\n",
      "Train Data Shape: (54463,)\n",
      "Cleaned Train Data Shape: (54463,)\n",
      "Validation Data Shape: (23256,)\n",
      "Test Data Shape: (23257,)\n",
      " \n",
      "Number of labels = 0 in train dataset as percentage: 9.13%\n",
      "Number of labels = 1 in train dataset as percentage: 13.54%\n",
      "Number of labels = 2 in train dataset as percentage: 14.43%\n",
      "Number of labels = 3 in train dataset as percentage: 17.22%\n",
      "Number of labels = 4 in train dataset as percentage: 45.68%\n",
      "Number of labels = -100 in train dataset as percentage: 0.00%\n",
      " \n",
      "Number of labels = 0 in val dataset as percentage: 19.63%\n",
      "Number of labels = 1 in val dataset as percentage: 20.27%\n",
      "Number of labels = 2 in val dataset as percentage: 20.42%\n",
      "Number of labels = 3 in val dataset as percentage: 19.81%\n",
      "Number of labels = 4 in val dataset as percentage: 19.88%\n",
      "Number of labels = -100 in val dataset as percentage: 0.00%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Load datasets\n",
    "train_data = pd.read_csv('km_combined_submission.csv') # we can use the k-means or gmm labeled data\n",
    "val_data = pd.read_csv('val.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Define preprocessing functions\n",
    "def rem_URL(sample):\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "def rem_tokens(sample):\n",
    "    sample_new = re.sub(r\"#\", \"\", sample)\n",
    "    sample_new = re.sub(r\"@\", \"\", sample_new)\n",
    "    return sample_new\n",
    "\n",
    "def preprocess_str(inp_string):\n",
    "    new_string = re.sub(r'@\\w+', '@USER', inp_string)  # Replace mentions with '@USER'\n",
    "    new_string = rem_URL(new_string)                   # Remove URLs\n",
    "    new_string = rem_tokens(new_string)                # Remove # and @\n",
    "    new_string = new_string.lower()                    # Convert to lowercase\n",
    "    words = new_string.split()\n",
    "    words = [w for w in words if w not in stopwords.words(\"english\")]  # Remove stopwords\n",
    "    lemmed = [WordNetLemmatizer().lemmatize(w) for w in words]         # Lemmatize\n",
    "    new_string = ' '.join(lemmed)\n",
    "    new_string = new_string.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    return new_string\n",
    "\n",
    "# Preprocess function for dataframe\n",
    "def preprocess(df):\n",
    "    new_df = df.copy()\n",
    "    new_df['Phrase'] = new_df['Phrase'].apply(lambda x: preprocess_str(x) if isinstance(x, str) else x)\n",
    "    return new_df\n",
    "\n",
    "# Get all train data (labeled and unlabeled)\n",
    "X_train = train_data['Phrase']\n",
    "y_train = train_data['Sentiment']\n",
    "\n",
    "# Get only labeled train data\n",
    "mask = (y_train != -100)\n",
    "train_data_clean = train_data[mask]\n",
    "X_train_clean = X_train[mask]\n",
    "y_train_clean = y_train[mask]\n",
    "\n",
    "# Get validation and test data\n",
    "X_val = val_data['Phrase']\n",
    "y_val = val_data['Sentiment']\n",
    "X_test = test_data['Phrase']\n",
    "\n",
    "# Preprocess train, validation, and test datasets, and drop NaNs if any\n",
    "X_train_clean_prep = preprocess(X_train_clean.to_frame()).dropna()\n",
    "X_val_prep = preprocess(X_val.to_frame()).dropna()\n",
    "X_test_prep = preprocess(X_test.to_frame()).dropna()\n",
    "\n",
    "# Check for any NaN values in each processed dataset\n",
    "print(f\"Number of NaN values in X_train_clean_prep['Phrase']: {X_train_clean_prep['Phrase'].isna().sum()}\")\n",
    "print(f\"Number of NaN values in X_val_prep['Phrase']: {X_val_prep['Phrase'].isna().sum()}\")\n",
    "print(f\"Number of NaN values in X_test_prep['Phrase']: {X_test_prep['Phrase'].isna().sum()}\")\n",
    "\n",
    "# Display data shapes and label distribution\n",
    "print(f\"Train Data Shape: {X_train.shape}\")\n",
    "print(f\"Cleaned Train Data Shape: {train_data_clean['Phrase'].shape}\")\n",
    "print(f\"Validation Data Shape: {X_val.shape}\")\n",
    "print(f\"Test Data Shape: {X_test.shape}\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 0 in train dataset as percentage: {((y_train == 0).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 1 in train dataset as percentage: {((y_train == 1).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in train dataset as percentage: {((y_train == 2).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 3 in train dataset as percentage: {((y_train == 3).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 4 in train dataset as percentage: {((y_train == 4).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = -100 in train dataset as percentage: {((y_train == -100).sum() / (X_train.shape[0])) * 100:0.2f}%\")\n",
    "\n",
    "print(\" \")\n",
    "print(f\"Number of labels = 0 in val dataset as percentage: {((y_val == 0).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 1 in val dataset as percentage: {((y_val == 1).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 2 in val dataset as percentage: {((y_val == 2).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 3 in val dataset as percentage: {((y_val == 3).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = 4 in val dataset as percentage: {((y_val == 4).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n",
    "print(f\"Number of labels = -100 in val dataset as percentage: {((y_val == -100).sum() / (X_val.shape[0])) * 100:0.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c54079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_clean = y_train[X_train_clean_prep.index] \n",
    "y_val = y_val[X_val_prep.index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742fcd36",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "812e4ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7231397849462365\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.49      0.63      4564\n",
      "           1       0.79      0.51      0.62      4713\n",
      "           2       0.98      0.93      0.95      4744\n",
      "           3       0.78      0.76      0.77      4605\n",
      "           4       0.49      0.92      0.64      4624\n",
      "\n",
      "    accuracy                           0.72     23250\n",
      "   macro avg       0.79      0.72      0.72     23250\n",
      "weighted avg       0.79      0.72      0.72     23250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Vectorize the text data using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train_clean_prep['Phrase'])\n",
    "X_val_vec = vectorizer.transform(X_val_prep['Phrase'])\n",
    "\n",
    "# Initialize and train the Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_vec, y_train_clean)\n",
    "\n",
    "# Make predictions on the validation set\n",
    "y_pred = rf_model.predict(X_val_vec)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Validation Accuracy:\", accuracy_score(y_val, y_pred))\n",
    "print(classification_report(y_val, y_pred))\n",
    "\n",
    "# If needed, make predictions on the test data\n",
    "X_test_vec = vectorizer.transform(X_test_prep['Phrase'])\n",
    "test_predictions = rf_model.predict(X_test_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08d18e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Validation Accuracy: 0.7329892473118279\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Vectorize the text data using CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_vec = vectorizer.fit_transform(X_train_clean_prep['Phrase'])\n",
    "X_val_vec = vectorizer.transform(X_val_prep['Phrase'])\n",
    "X_test_vec = vectorizer.transform(X_test_prep['Phrase'])\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg_model.fit(X_train_vec, y_train_clean)\n",
    "y_pred_log_reg = log_reg_model.predict(X_val_vec)\n",
    "print(\"Logistic Regression Validation Accuracy:\", accuracy_score(y_val, y_pred_log_reg))\n",
    "\n",
    "test_predictions_log_reg = log_reg_model.predict(X_test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf107485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Validation Accuracy: 0.7269247311827957\n",
      "Logistic Regression Validation Accuracy: 0.7589247311827957\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the text data using CountVectorizer\n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "X_train_vec = vectorizer.fit_transform(X_train_clean_prep['Phrase'])\n",
    "X_val_vec = vectorizer.transform(X_val_prep['Phrase'])\n",
    "X_test_vec = vectorizer.transform(X_test_prep['Phrase'])\n",
    "\n",
    "# Random Forest Classifier\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_vec, y_train_clean)\n",
    "y_pred_rf = rf_model.predict(X_val_vec)\n",
    "print(\"Random Forest Validation Accuracy:\", accuracy_score(y_val, y_pred_rf))\n",
    "\n",
    "# Logistic Regression\n",
    "log_reg_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg_model.fit(X_train_vec, y_train_clean)\n",
    "y_pred_log_reg = log_reg_model.predict(X_val_vec)\n",
    "print(\"Logistic Regression Validation Accuracy:\", accuracy_score(y_val, y_pred_log_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "296e53fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression with L2 Regularization Validation Accuracy: 0.9943225806451613\n",
      "Logistic Regression with L1 Regularization Validation Accuracy: 0.9575913978494623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# L2 Regularization (Ridge)\n",
    "log_reg_model_l2 = LogisticRegression(random_state=42, max_iter=1000, penalty='l2', C=1.0)\n",
    "log_reg_model_l2.fit(X_train_vec, y_train_clean)\n",
    "y_pred_log_reg_l2 = log_reg_model_l2.predict(X_val_vec)\n",
    "print(\"Logistic Regression with L2 Regularization Validation Accuracy:\", accuracy_score(y_val, y_pred_log_reg_l2))\n",
    "\n",
    "# L1 Regularization (Lasso)\n",
    "log_reg_model_l1 = LogisticRegression(random_state=42, max_iter=1000, penalty='l1', solver='liblinear', C=1.0)\n",
    "log_reg_model_l1.fit(X_train_vec, y_train_clean)\n",
    "y_pred_log_reg_l1 = log_reg_model_l1.predict(X_val_vec)\n",
    "print(\"Logistic Regression with L1 Regularization Validation Accuracy:\", accuracy_score(y_val, y_pred_log_reg_l1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "568d8c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_prep = preprocess(X_test.to_frame())\n",
    "X_test_prep['Phrase'] = X_test_prep['Phrase'].fillna('')\n",
    "X_test_vec = vectorizer.transform(X_test_prep['Phrase'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e059079d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23257"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "84cb83df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Test Predictions: [3 2 4 ... 2 3 1]\n"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "y_test_pred_log_reg = log_reg_model.predict(X_test_vec)\n",
    "\n",
    "# Print test predictions if needed\n",
    "print(\"Logistic Regression Test Predictions:\", y_test_pred_log_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f2b43",
   "metadata": {},
   "source": [
    "### Logistic Regression with an augmented KMeans training dataset preformed best: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d8c00093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to 'logistic_regression_test_predictions_km.csv'\n"
     ]
    }
   ],
   "source": [
    "# Generate PhraseID as a sequence from 0 to the number of predictions\n",
    "phrase_ids = range(len(y_test_pred_log_reg))\n",
    "\n",
    "# Create a DataFrame with PhraseID and Sentiment columns\n",
    "test_predictions_df = pd.DataFrame({\n",
    "    'PhraseID': phrase_ids,\n",
    "    'Sentiment': y_test_pred_log_reg\n",
    "})\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "test_predictions_df.to_csv('logistic_regression_test_predictions_km.csv', index=False)\n",
    "\n",
    "print(\"Predictions saved to 'logistic_regression_test_predictions_km.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1b9d99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
